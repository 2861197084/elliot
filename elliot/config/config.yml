experiment:
  data_paths:
    train_data: ../data/{0}/trainingset.tsv
    validation_data: ../data/{0}/validation.tsv
    test_data: ../data/{0}/testset.tsv
    feature_data: ../data/{0}/original/features.npy
  path_output_rec_result: ../results/{0}/recs/
  path_output_rec_weight: ../results/{0}/weights/
  path_output_rec_performance: ../results/{0}/performance/
  path_logger_config: ./config/logger_config.yml
  path_log_folder: ../log/
  dataset: example
  top_k: 50
  metrics: [nDCG, Precision, Recall, ItemCoverage]
  relevance: 1
  gpu: -1 # -1 is not use GPU
  models:
#    MultiDAE:
#      meta:
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#        validation_rate: 10
#        verbose: False
#        save_weights: True
#        save_recs: True
#      lr: 0.001
#      epochs: 50
#      intermediate_dim: 600
#      latent_dim: 200
#      batch_size: -1
#      dropout_pkeep: 1
#      reg_lambda: 0.01
    BPRMF:
      meta:
        hyper_max_evals: 10
        hyper_opt_alg: tpe
        verbose: 1
        save_weights: True
        save_recs: True
        validation_metric: nDCG
        restore_epoch: 2
      lr: 0.05
      epochs: 10
      embed_k: [10,50,100,125,150,200]
      bias_regularization: 0
      user_regularization: [0.0025,0.005,0.01]
      positive_item_regularization: [0.0025,0.005,0.01]
      negative_item_regularization: [0.00025,0.0005,0.001]
      update_negative_item_factors: True
      update_users: True
      update_items: True
      update_bias: True
    NNBPRMF:
      meta:
        hyper_max_evals: 10
        hyper_opt_alg: tpe
        verbose: 1
        save_weights: True
        save_recs: True
        validation_metric: nDCG
        restore_epoch: -1
      lr: [0.005,0.05,0.0005]
      epochs: 20
      embed_k: [10,50,100,125,150,200]
      batch_size: 512
      l_w: 0.000025
      l_b: 0
    VBPR:
      meta:
        verbose: 1
        save_weights: True
        save_recs: True
        validation_metric: nDCG
        restore_epoch: -1
      lr: 0.005
      epochs: 20
      embed_k: 100
      embed_d: 20
      batch_size: 128
      l_w: 0.000025
      l_b: 0
      l_e: 0.002